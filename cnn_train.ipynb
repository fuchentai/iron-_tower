{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strat 1586221708.509327\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "basefile=os.getcwd()\n",
    "start=time.time()\n",
    "print(\"strat %f\"%start)\n",
    "train_data=pd.read_csv(os.path.join(basefile,'train_data1.csv'), encoding='utf-8')\n",
    "train_label=pd.read_csv(os.path.join(basefile,'train_label1.csv'), encoding='utf-8')\n",
    "# val_data=pd.read_csv(os.path.join(basefile,'val_data.csv'), encoding='utf-8')\n",
    "# val_label=pd.read_csv(os.path.join(basefile,'val_label.csv'), encoding='utf-8')\n",
    "\n",
    "#normalize\n",
    "train_data = np.array(train_data)\n",
    "train_data /= 255.0\n",
    "train_data = train_data.astype(float)\n",
    "train_data=train_data[:,1:]\n",
    "end=time.time()\n",
    "print(\"end %f\"%end)\n",
    "# val_data = np.array(val_data)\n",
    "# val_data /= 255.0\n",
    "# val_data = val_data.astype(float)\n",
    "# val_data = val_data[:,1:]\n",
    "\n",
    "print(train_data.shape)\n",
    "plt.imshow(train_data[10,:].reshape((128,64,3)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels=['%', '&', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'x', 'y', 'z']\n",
    "plt.imshow(train_data[916,:].reshape((128,64,3)))\n",
    "plt.show()\n",
    "def change_one_hot_label(X):\n",
    "    T=np.zeros((X.shape[0],36))\n",
    "    print(X)\n",
    "    for idx,row in enumerate(T): \n",
    "        row[all_labels.index(X[idx,1])]=1\n",
    "    return T\n",
    "train_label=np.array(train_label)\n",
    "tr_label=change_one_hot_label(train_label)\n",
    "#va_label=_change_one_hot_label(val_label)\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "max_steps = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(input_tensor,regularizer,avg_class,resuse):\n",
    "    #创建第一个卷积层，得到特征图大小为32@28x28\n",
    "    with tf.variable_scope(\"C1-conv\",reuse=resuse):\n",
    "        conv1_weights = tf.get_variable(\"weight\", [3, 3, 3, 32],\n",
    "                             initializer=tf.random_normal_initializer(mean=0.0,stddev=1.0,dtype=tf.float32))\n",
    "        conv1_biases = tf.get_variable(\"bias\", [32], initializer=tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu1 = tf.nn.relu(tf.layers.batch_normalization(tf.nn.bias_add(conv1, conv1_biases)))\n",
    "    #创建第一个池化层，池化后的结果为32@32*16\n",
    "    with tf.name_scope(\"S2-max_pool\",):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "    # 创建第二个卷积层，得到特征图大小为64@14x14。注意，第一个池化层之后得到了32个\n",
    "    # 特征图，所以这里设输入的深度为32，我们在这一层选择的卷积核数量为64，所以输出\n",
    "    # 的深度是64，也就是说有64个特征图\n",
    "    with tf.variable_scope(\"C3-conv\",reuse=resuse):\n",
    "        conv2_weights = tf.get_variable(\"weight\", [3, 3, 32, 64],\n",
    "                                     initializer=tf.random_normal_initializer(mean=0.0,stddev=1.0,dtype=tf.float32))\n",
    "        conv2_biases = tf.get_variable(\"bias\", [64], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu2 = tf.nn.relu(tf.layers.batch_normalization(tf.nn.bias_add(conv2, conv2_biases)))\n",
    "    #创建第二个池化层，池化后结果为64@8*16\n",
    "    with tf.name_scope(\"S4-max_pool\",):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        #get_shape()函数可以得到这一层维度信息，由于每一层网络的输入输出都是一个batch的矩阵，\n",
    "        #所以通过get_shape()函数得到的维度信息会包含这个batch中数据的个数信息\n",
    "        #shape[1]是长度方向，shape[2]是宽度方向，shape[3]是深度方向\n",
    "        #shape[0]是一个batch中数据的个数，reshape()函数原型reshape(tensor,shape,name)\n",
    "        shape = pool2.get_shape().as_list()\n",
    "        nodes = shape[1] * shape[2] * shape[3]    #nodes=8192\n",
    "        reshaped = tf.reshape(pool2, [-1, nodes])\n",
    "    #创建第一个全连层\n",
    "    with tf.variable_scope(\"layer5-full1\",reuse=resuse):\n",
    "        Full_connection1_weights = tf.get_variable(\"weight\", [nodes, 512],\n",
    "                                      initializer=tf.random_normal_initializer(mean=0.0,stddev=1.0,dtype=tf.float32))\n",
    "        #if regularizer != None:\n",
    "        tf.add_to_collection(\"losses\", regularizer(Full_connection1_weights))\n",
    "        Full_connection1_biases = tf.get_variable(\"bias\", [512],\n",
    "                                                     initializer=tf.constant_initializer(0.0))\n",
    "        Full_1 = tf.nn.relu(tf.layers.batch_normalization(tf.matmul(reshaped, Full_connection1_weights) + \\\n",
    "                                                                   Full_connection1_biases))\n",
    "    #创建第二个全连层\n",
    "    with tf.variable_scope(\"layer6-full2\",reuse=resuse):\n",
    "        Full_connection2_weights = tf.get_variable(\"weight\", [512, 36],\n",
    "                                      initializer=tf.random_normal_initializer(mean=0.0,stddev=1.0,dtype=tf.float32))\n",
    "        #if regularizer != None:\n",
    "        tf.add_to_collection(\"losses\", regularizer(Full_connection2_weights))\n",
    "        Full_connection2_biases = tf.get_variable(\"bias\", [36],\n",
    "                                                   initializer=tf.constant_initializer(0.0))\n",
    "        result = tf.matmul(Full_1, Full_connection2_weights) + Full_connection2_biases\n",
    "    \n",
    "    return result\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None ,128,64,3],name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 36], name=\"y-input\")\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.001)\n",
    "y = hidden_layer(x,regularizer,avg_class=None,resuse=False)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)#, global_step=training_step\n",
    "\n",
    "with tf.control_dependencies([train_step]):#, variables_averages_op\n",
    "    train_op = tf.no_op(name='train')\n",
    "crorent_predicition = tf.equal(tf.arg_max(y,1),tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(crorent_predicition,tf.float32))\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "train_acc_list=[]\n",
    "train_loss=[]\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        batch_mask = np.random.choice(14312, batch_size,replace=False)\n",
    "        x_batch = train_data[batch_mask].reshape((-1,128,64,3))\n",
    "        y_batch = tr_label[batch_mask]\n",
    "\n",
    "        _, loss_value, train_acc = sess.run([train_op, loss, accuracy],\n",
    "                                            feed_dict={x: x_batch, y_: y_batch})\n",
    "        train_acc_list.append(train_acc)\n",
    "        train_loss.append(loss_value)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After %d training steps,train accuracy %g%%\" % (i, train_acc * 100))\n",
    "            saver.save(sess,r\"E:\\DeepLearning\\2020_4\\saver\\cnn_trian.ckpt\")\n",
    "\n",
    "plt.plot(train_loss, markevery=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
